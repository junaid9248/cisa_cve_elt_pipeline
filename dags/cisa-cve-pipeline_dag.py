# Operator: These are the nodes in a DAG, they represent a task.
# Dependencies: These are specified relationships between operators.
# Tasks: In airflow, a task is the unit of work.
# Task instances: Execution of a task at a specific point in time ie: in a DagRun

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

from src.config import PYTHONPATH, GCLOUD_PROJECTNAME, IS_LOCAL
import logging 
import os
from datetime import timedelta
import pendulum as pend


GH_TOKEN = os.environ.get('GH_TOKEN') 
GCLOUD_BUCKETNAME = os.environ.get('GCLOUD_BUCKETNAME')
GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
local_tz = pend.timezone('America/Chicago')
start_date = pend.datetime(2025, 12, 25, tz=local_tz)
# Define the DAG default args
default_args = {
    'owner': 'junaid',
    'retries': 1,
    'start_date': start_date,
    'retry_delay': timedelta(seconds=30),
    # Adding a buffer window for tasks especially raws extraction task so it does NOT time out!
    'execution_timeout': timedelta(hours=2),
    'max_active_runs': 1,
}

env_dict = {
    'IS_LOCAL': str(IS_LOCAL), 
    'PYTHONPATH': PYTHONPATH,
    'GH_TOKEN': GH_TOKEN, 
    'GCLOUD_BUCKETNAME': GCLOUD_BUCKETNAME, 
    'GOOGLE_APPLICATION_CREDENTIALS': GOOGLE_APPLICATION_CREDENTIALS, 
    'GCLOUD_PROJECTNAME': GCLOUD_PROJECTNAME,
}

# Defining the DAG
with DAG(
    dag_id='cve_elt_pipeline',
    schedule ='@daily',
    default_args= default_args,
    description='Extract raw cve jsons from CISA Vulnrichment gihtub repo into GCS bucket -> Transform into structured formats and Load into BigQuery-> perform quality checks'

) as dag:
    # Defining the TASKS using operators
    # TASK 1: EXTRACT
    logging.info("Setting up the extract task")
    extract_raw_jsons = BashOperator(
        task_id = 'cve_raws_extraction',
        bash_command=(
            'cd /opt/airflow/repo && '
            'python main.py --cloud'
        ),
        env = env_dict
    )

    # TASK 2: TRANSFORM and LOAD
    logging.info("Setting up the transform and load task")
    transform_load_raw_json = BashOperator(
        task_id = 'cve_raws_transformation_loading',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.transform'
        ),
        env = env_dict
    )
    
    # TASK 3: LOAD FINAL TABLE TO BQ
    logging.info("Setting up the final load task")
    load_final_table = BashOperator(
        task_id = 'final_table_load',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.load_final'
        ),
        env = env_dict
    )

    # TASK 4: DATA QUALITY CHECKS
    logging.info("Setting up the data quality check task")
    data_quality_checks = BashOperator(
        task_id = 'final_data_quality_checks',
        bash_command=(
            'cd /opt/airflow/repo/tests ; '
            'python table_test.py'
    ),
        env = env_dict
    )

    # Defining the dependencies
    extract_raw_jsons >> transform_load_raw_json >> load_final_table >> data_quality_checks