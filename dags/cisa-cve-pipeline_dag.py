# Operator: These are the nodes in a DAG, they represent a task.
# Dependencies: These are specified relationships between operators.
# Tasks: In airflow, a task is the unit of work.
# Task instances: Execution of a task at a specific point in time ie: in a DagRun

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator


import logging 
from datetime import timedelta
import pendulum as pend
from config import IS_LOCAL, GH_TOKEN, GCLOUD_BUCKETNAME, GCLOUD_APP_CREDENTIALS, GCLOUD_PROJECTNAME, AIRFLOW_WEBSERVER_SECRET_KEY

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
local_tz = pend.timezone('America/Chicago')
start_date = pend.datetime(2025, 12, 19, tz=local_tz)
# Define the DAG default args
default_args = {
    'owner': 'junaid',
    'retries': 1,
    'start_date': start_date,
    'retry_delay': timedelta(minutes=1),
    'max_active_runs': 1,
}

env_dict = {
    'PYTHONPATH': '/opt/airflow/repo',
    'IS_LOCAL': str(IS_LOCAL), 
    'GH_TOKEN': GH_TOKEN, 
    'GCLOUD_BUCKETNAME': GCLOUD_BUCKETNAME, 
    'GCLOUD_APP_CREDENTIALS': GCLOUD_APP_CREDENTIALS, 
    'GCLOUD_PROJECTNAME': GCLOUD_PROJECTNAME,
}


# Defining the DAG
with DAG(
    dag_id='cve_elt_pipeline',
    schedule ='@daily',
    default_args= default_args,
    description='Extract raw cve jsons from CISA Vulnrichment gihtub repo into GCS bucket -> Transform into structured formats and Load into BigQuery-> perform quality checks'

) as dag:
    # Defining the TASKS using operators
    # TASK 1: EXTRACT
    logging.info("Setting up the extract task")
    extract_raw_jsons = BashOperator(
        task_id = 'cve_raws_extraction',
        bash_command=(
            'cd /opt/airflow/repo && '
            'python main.py --cloud 1999,2000,2001,2002'
        ),
        env = env_dict
    )

    # TASK 2: TRANSFORM and LOAD
    logging.info("Setting up the transform and load task")
    transform_load_raw_json = BashOperator(
        task_id = 'cve_raws_transformation_loading',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.transform 1999,2000,2001,2002'
        ),
        env = env_dict
    )

    # TASK 3: DATA QUALITY CHECKS
    logging.info("Setting up the data quality check task")
    data_quality_checks = BashOperator(
        task_id = 'final_data_quality_checks',
        bash_command=(
            'cd /opt/airflow/repo/tests ; '
            'python table_test.py'
    ),
        env = env_dict
    )

    # Defining the dependencies
    extract_raw_jsons >> transform_load_raw_json >> data_quality_checks