# Operator: These are the nodes in a DAG, they represent a task.
# Dependencies: These are specified relationships between operators.
# Tasks: In airflow, a task is the unit of work.
# Task instances: Execution of a task at a specific point in time ie: in a DagRun

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator


import logging 
from datetime import timedelta
from pendulum import datetime, date
from src.config import IS_LOCAL, GH_TOKEN, GCLOUD_BUCKETNAME, GCLOUD_APP_CREDENTIALS, GCLOUD_PROJECTNAME, MY_EMAIL

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

start_date = datetime(2025, 12, 16, tz="UTC")
# Define the DAG default args
default_args = {
    'owner': 'junaid',
    'retries': 3,
    'start_date': start_date,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

env_dict = {
    'PYTHONPATH': '/opt/airflow/dags/repo',
    'IS_LOCAL': IS_LOCAL, 
    'GH_TOKEN': GH_TOKEN, 
    'GCLOUD_BUCKETNAME': GCLOUD_BUCKETNAME, 
    'GCLOUD_APP_CREDENTIALS': GCLOUD_APP_CREDENTIALS, 
    'GCLOUD_PROJECTNAME': GCLOUD_PROJECTNAME,
}


# Defining the DAG
with DAG(
    dag_id='cve_elt_pipeline',
    schedule_interval ='@daily',
    default_args= default_args,
    description='Extract raw cve jsons from CISA Vulnrichment gihtub repo into GCS bucket -> Transform into structured formats and Load into BigQuery-> perform quality checks'

) as dag:
    # Defining the TASKS using operators
    # TASK 1: EXTRACT
    extract_raw_jsons = BashOperator(
        task_id = 'cve_raws_extraction',
        bash_command=(
            'cd /opt/airflow/dags/repo &&'
            'python -m src.main --cloud'
        ),
        env = env_dict
    ),

    # TASK 2: TRANSFORM and LOAD
    transform_load_raw_json = BashOperator(
        task_id = 'cve_raws_transformation_loading',
        bash_command=(
            'cd /opt/airflow/dags/repo &&'
            'python -m src.transform'
        ),
        env = env_dict
    )

    # TASK 3: DATA QUALITY CHECKS
    data_quality_checks = BashOperator(
        task_id = 'final_data_quality_checks',
        bash_command=(
            'cd /opt/airflow/dags/repo &&'
            'python -m tests.table_test'
    ),
        env = env_dict
    )

    # Defining the dependencies
    extract_raw_jsons >> transform_load_raw_json >> data_quality_checks