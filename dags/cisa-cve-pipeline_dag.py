# Operator: These are the nodes in a DAG, they represent a task.
# Dependencies: These are specified relationships between operators.
# Tasks: In airflow, a task is the unit of work.
# Task instances: Execution of a task at a specific point in time ie: in a DagRun

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

from src.config import PYTHONPATH, GCLOUD_PROJECTNAME, IS_LOCAL
import logging 
import os
from datetime import timedelta
import pendulum as pend


GH_TOKEN = os.environ.get('GH_TOKEN') 
GCLOUD_BUCKETNAME = os.environ.get('GCLOUD_BUCKETNAME')
GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

local_tz = pend.timezone('America/Chicago')
start_date = pend.datetime(2025, 12, 30, tz=local_tz)
# Define the DAG default args
default_args = {
    'owner': 'junaid',
    'retries': 1,
    'start_date': start_date,
    'retry_delay': timedelta(minutes = 5),
    # Adding a buffer window for tasks especially raws extraction task so it does NOT time out!
    'execution_timeout': timedelta(hours=4),
    'max_active_runs': 1,
}

env_dict = {
    'IS_LOCAL': str(IS_LOCAL), 
    'PYTHONPATH': PYTHONPATH,
    'GH_TOKEN': GH_TOKEN, 
    'GCLOUD_BUCKETNAME': GCLOUD_BUCKETNAME, 
    'GOOGLE_APPLICATION_CREDENTIALS': GOOGLE_APPLICATION_CREDENTIALS, 
    'GCLOUD_PROJECTNAME': GCLOUD_PROJECTNAME,
}
early_years = ['1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020']
early_str = ','.join(early_years)
new_years = ['2021','2022','2023','2024','2025']
new_str = ','.join(new_years)

# Defining the DAG
with DAG(
    dag_id='cve_elt_pipeline',
    schedule ='@daily',
    default_args= default_args,
    description='Extract raw cve jsons from CISA Vulnrichment gihtub repo into GCS bucket -> Transform into structured table formats -> Load into BigQuery-> perform quality checks'

) as dag:
    #2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025
    # Defining the TASKS using operators
    # TASK 1 and 2: EXTRACT
    logging.info("Setting up the extract task")
    extract_raw_jsons_old = BashOperator(
        task_id = 'cve_raws_extraction_older_years',
        bash_command=(
            'cd /opt/airflow/repo && '
            f'python main.py --cloud {early_str}'
        ),
        env = env_dict
    )

    extract_raw_jsons_new = BashOperator(
        task_id = 'cve_raws_extraction_newer_years',
        bash_command=(
            'cd /opt/airflow/repo && '
            f'python main.py --cloud {new_str}'
        ),
        env = env_dict
    )

    # TASK 3: TRANSFORM and LOAD
    logging.info("Setting up the transform and staging task")
    transform_load_raw_json = BashOperator(
        task_id = 'cve_raws_transformation_loading',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.transform'
        ),
        env = env_dict
    )
    
    # TASK 4: LOAD FINAL TABLE TO BQ
    logging.info("Setting up the final load task")
    load_final_table = BashOperator(
        task_id = 'final_table_load',
        bash_command=(
            'cd /opt/airflow/repo ; '
            'python -m src.load_final'
        ),
        env = env_dict
    )

    # TASK 5: DATA QUALITY CHECKS
    logging.info("Setting up the data quality check task")
    data_quality_checks = BashOperator(
        task_id = 'final_data_quality_checks',
        bash_command=(
            'cd /opt/airflow/repo/tests ; '
            'python table_test.py'
    ),
        env = env_dict
    )

    # Defining the dependencies
    extract_raw_jsons_old >> extract_raw_jsons_new >>transform_load_raw_json >> load_final_table >> data_quality_checks