# docker-compose.yml

services:
  #Service 1: PostgreSQL Database
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    #secrets: To be added for production

  #Service 2: Airflow Webserver
  airflow-webserver:
    build: .
    
    image: cve-airflow:latest
    depends_on:
      - postgres
    working_dir: /opt/airflow/repo
    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/repo/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      # For later version of Airflow, default auth manager is SimpleAuthManager so we change it to FabAuthManager
      AIRFLOW__CORE__AUTH_MANAGER: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"
      AIRFLOW__CORE__HOSTNAME_CALLABLE: "airflow.utils.net.get_host_ip_address"
      
      IS_LOCAL: 'false'
      GCLOUD_PROJECTNAME: "cisa-cve-data-pipeline"
      GOOGLE_APPLICATION_CREDENTIALS: 'secrets/cisa-cve-data-pipeline-a26a62e94ef3.json'
      PYTHONPATH: /opt/airflow/repo
      GH_TOKEN: ${GH_TOKEN}

      # Docker Compose automatically loads these from .env
      GCLOUD_BUCKETNAME: ${GCLOUD_BUCKETNAME}
      MY_EMAIL: ${MY_EMAIL}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    ports:
      - "8080:8080"
    volumes:
      - ./:/opt/airflow/repo
    command: bash -c "
                      airflow db migrate && 
                      airflow users create 
                      --username airflow 
                      --password airflow 
                      --firstname Air 
                      --lastname Flow 
                      --role Admin 
                      --email junaidwork456@gmail.com
                      || true
                      && airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5


  #Service 3: Airflow Scheduler
  airflow-scheduler:
    build: .
    image: cve-airflow:latest
    depends_on:
      - postgres
    
    working_dir: /opt/airflow/repo

    environment:
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/repo/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__AUTH_MANAGER: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__HOSTNAME_CALLABLE: "airflow.utils.net.get_host_ip_address"
      # Non-critcal env vars
      IS_LOCAL: 'false'
      GCLOUD_PROJECTNAME: "cisa-cve-data-pipeline"
      GOOGLE_APPLICATION_CREDENTIALS: 'secrets/cisa-cve-data-pipeline-a26a62e94ef3.json'
      PYTHONPATH: /opt/airflow/repo  

      # Docker Compose automatically loads these from .env
      GH_TOKEN: ${GH_TOKEN}
      GCLOUD_BUCKETNAME: ${GCLOUD_BUCKETNAME}
      MY_EMAIL: ${MY_EMAIL}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}

    volumes: 
      - ./:/opt/airflow/repo
    command: airflow scheduler



volumes:
  postgres_data:
